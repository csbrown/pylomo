{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import local_models.local_models\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "from ml_battery.utils import cmap\n",
    "import matplotlib as mpl\n",
    "import logging\n",
    "import ml_battery.log\n",
    "import time\n",
    "import os\n",
    "import functools\n",
    "import collections\n",
    "import itertools\n",
    "import pymongo\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "import local_models.loggin\n",
    "import local_models.tf_w2v_models\n",
    "import pickle\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "np.random.seed(1)\n",
    "#reload(local_models.loggin)\n",
    "#reload(local_models.TLS_models)\n",
    "np.warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = [8.0, 8.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = 1\n",
    "BANDWIDTH = 0.35\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "KERNEL=local_models.local_models.GaussianKernel\n",
    "root_dir = \"/home/scott/local_w2v/\"\n",
    "project_dir = os.path.join(root_dir, \"r{:03d}_k{}\".format(RUN, KERNEL(bandwidth=BANDWIDTH)))\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "os.makedirs(project_dir, exist_ok=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_by_regex(text, regexp):\n",
    "    return regexp.sub(\"\", text)\n",
    "\n",
    "url_re = re.compile(r\"http.?://[^\\s]+[\\s]?\")\n",
    "def remove_urls(text):\n",
    "    return remove_by_regex(text, url_re)\n",
    "\n",
    "specialchar_re = re.compile(\"[\" + ''.join(map(re.escape, \n",
    "    [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\", \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "     \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\", \"!\", \"?\", \".\", \"'\", \"--\", \"---\"])) + \"]\")\n",
    "def remove_special_chars(text):\n",
    "    return remove_by_regex(text, specialchar_re)\n",
    "\n",
    "username_re = re.compile(r\"@[^\\s]+[\\s]?\")\n",
    "def remove_usernames(text):\n",
    "    return remove_by_regex(text, username_re)\n",
    "\n",
    "number_re = re.compile(r\"\\s?[0-9]+\\.?[0-9]*\")\n",
    "def remove_numbers(text):\n",
    "    return remove_by_regex(text, number_re)\n",
    "    \n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "cleanup = (remove_urls,\n",
    "           remove_usernames,\n",
    "           remove_special_chars,\n",
    "           remove_numbers,\n",
    "           lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cikm_tweet_column(line):\n",
    "    try:\n",
    "        return line.split(\"\\t\")[-2]\n",
    "    except Exception as e:\n",
    "        return None\n",
    "def count(iterable, item):\n",
    "    tot = 0\n",
    "    for i in iterable:\n",
    "        if i == item:\n",
    "            tot += 1\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_training_data = os.path.join(data_dir, \"training_set_tweets.txt\")\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tweets = []\n",
    "with open(pre_training_data) as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            tweet = get_cikm_tweet_column(line)\n",
    "            for cleaner in cleanup:\n",
    "                tweet = cleaner(tweet)\n",
    "            tweets.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "print(len(tweets), count(tweets, None))\n",
    "tweets = [i for i in tweets if i is not None]\n",
    "print(len(tweets))\n",
    "toknized_tweets = [tknzr.tokenize(t) for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fuckit(object):\n",
    "    def __init__(self, message=None):\n",
    "        self.message = message\n",
    "    def __enter__(self): return self\n",
    "    def __exit__(self, *args):\n",
    "        if self.message is not None:\n",
    "            print(self.message)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing to del\n",
      "nothing to del\n",
      "nothing to del\n",
      "nothing to del\n",
      "nothing to del\n"
     ]
    }
   ],
   "source": [
    "with fuckit(\"nothing to del\"):\n",
    "    TFGlobalSesh.close()\n",
    "with fuckit(\"nothing to del\"):\n",
    "    del TFGlobalSesh \n",
    "with fuckit(\"nothing to del\"):\n",
    "    with TFGlobalGraph.as_default():\n",
    "        tf.reset_default_graph()\n",
    "with fuckit(\"nothing to del\"):\n",
    "    del TFGlobalGraph \n",
    "with fuckit(\"nothing to del\"):\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 6000\n",
    "BASE_MODEL_EPOCHS = 10000\n",
    "BASE_MODEL_RUN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(os.path.join(root_dir, \"base_model_v{:08d}_e{:08d}_r{:03d}.model\".format(VOCAB_SIZE, BASE_MODEL_EPOCHS, BASE_MODEL_RUN)), 'rb') as f:\n",
    "        pmodel = pickle.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    pmodel = Word2Vec(vocabulary_size=VOCAB_SIZE, epochs=BASE_MODEL_EPOCHS, log_epochs=0.01)\n",
    "    pmodel.fit(toknized_tweets, sample_weight=np.ones(len(toknized_tweets)))\n",
    "    import pickle\n",
    "    with open(os.path.join(root_dir, \"base_model_v{:08d}_e{:08d}_r{:03d}.model\".format(VOCAB_SIZE, BASE_MODEL_EPOCHS, BASE_MODEL_RUN)), 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sweet',\n",
       " 'yummy',\n",
       " 'lovely',\n",
       " 'delicious',\n",
       " 'nice',\n",
       " 'beautiful',\n",
       " 'cool',\n",
       " 'gorgeous',\n",
       " 'tasty',\n",
       " 'cute',\n",
       " 'funny',\n",
       " 'rare',\n",
       " 'awesome',\n",
       " 'wet',\n",
       " 'fabulous',\n",
       " 'adorable',\n",
       " 'sick',\n",
       " 'goodnight',\n",
       " 'similar',\n",
       " 'wonderful',\n",
       " 'familiar',\n",
       " 'neat',\n",
       " 'sad',\n",
       " 'weird',\n",
       " 'perfect',\n",
       " 'good',\n",
       " 'happy',\n",
       " 'sexy',\n",
       " 'welcome',\n",
       " 'rad',\n",
       " 'warm',\n",
       " 'bff',\n",
       " 'strange',\n",
       " 'goood',\n",
       " 'ricky',\n",
       " 'bright',\n",
       " 'dope',\n",
       " 'gd',\n",
       " 'fly',\n",
       " 'romantic',\n",
       " 'lonely',\n",
       " 'boring',\n",
       " 'aww',\n",
       " 'lucky',\n",
       " 'badass',\n",
       " 'jealous',\n",
       " 'nasty',\n",
       " 'fresh',\n",
       " 'windy',\n",
       " 'sleepy',\n",
       " 'glorious',\n",
       " 'amazing',\n",
       " 'impressive',\n",
       " 'xoxo',\n",
       " 'creepy',\n",
       " 'chill',\n",
       " 'salad',\n",
       " 'disgusting',\n",
       " 'honey',\n",
       " 'cheap',\n",
       " 'scary',\n",
       " 'awww',\n",
       " 'loves',\n",
       " 'proud',\n",
       " 'wicked',\n",
       " 'lame',\n",
       " 'clever',\n",
       " 'mmm',\n",
       " 'hot',\n",
       " 'dry',\n",
       " 'spicy',\n",
       " 'bomb',\n",
       " 'hilarious',\n",
       " 'priceless',\n",
       " 'tgif',\n",
       " 'friendly',\n",
       " 'easy',\n",
       " 'pleased',\n",
       " 'blessed',\n",
       " 'painful',\n",
       " 'loving',\n",
       " 'safe',\n",
       " 'dangerous',\n",
       " 'fantastic',\n",
       " 'helpful',\n",
       " 'alright',\n",
       " 'silly',\n",
       " 'annoying',\n",
       " 'stoked',\n",
       " 'kisses',\n",
       " 'dirty',\n",
       " 'fab',\n",
       " 'chilly',\n",
       " 'incredible',\n",
       " '#threewordsaftersex',\n",
       " 'glory',\n",
       " 'bummer',\n",
       " 'corn',\n",
       " 'stunning',\n",
       " 'famous',\n",
       " 'wow',\n",
       " 'tired',\n",
       " 'hello',\n",
       " 'permanent',\n",
       " 'poor',\n",
       " 'flies',\n",
       " 'important',\n",
       " 'hahaha',\n",
       " 'fun',\n",
       " 'soft',\n",
       " 'original',\n",
       " 'exciting',\n",
       " 'cutest',\n",
       " 'virgin',\n",
       " 'ideal',\n",
       " 'worthy',\n",
       " 'loss',\n",
       " '#tl',\n",
       " 'relaxing',\n",
       " 'epic',\n",
       " 'terrible',\n",
       " 'soon',\n",
       " 'xo',\n",
       " 'long',\n",
       " 'flight',\n",
       " 'own',\n",
       " 'homemade',\n",
       " 'julia',\n",
       " 'rough',\n",
       " 'fries',\n",
       " 'finest',\n",
       " 'blue',\n",
       " 'inspiring',\n",
       " 'genius',\n",
       " 'sore',\n",
       " 'soup',\n",
       " 'real',\n",
       " 'dear',\n",
       " 'baked',\n",
       " 'simple',\n",
       " 'ahh',\n",
       " 'grown',\n",
       " 'newest',\n",
       " 'inspirational',\n",
       " 'wise',\n",
       " 'interesting',\n",
       " 'thin',\n",
       " 'cake',\n",
       " 'enjoying',\n",
       " 'couch',\n",
       " 'naughty',\n",
       " 'fever',\n",
       " 'fine',\n",
       " 'mask',\n",
       " 'tiny',\n",
       " 'derby',\n",
       " 'relax',\n",
       " 'chips',\n",
       " 'bliss',\n",
       " 'brutal',\n",
       " 'hugs',\n",
       " 'legit',\n",
       " 'yellow',\n",
       " 'true',\n",
       " 'penis',\n",
       " 'proper',\n",
       " 'equal',\n",
       " 'dam',\n",
       " 'ummm',\n",
       " 'ohh',\n",
       " '#aintnothinglike',\n",
       " 'bad',\n",
       " 'smile',\n",
       " 'quiet',\n",
       " 'talented',\n",
       " 'fiesta',\n",
       " 'intense',\n",
       " 'vodka',\n",
       " 'fancy',\n",
       " 'sharp',\n",
       " 'thank',\n",
       " 'much',\n",
       " 'avatar',\n",
       " 'gooo',\n",
       " 'healthy',\n",
       " 'exhausted',\n",
       " 'angry',\n",
       " 'doll',\n",
       " 'brilliant',\n",
       " 'daddy',\n",
       " 'false',\n",
       " 'golden',\n",
       " 'goodbye',\n",
       " 'tall',\n",
       " 'screen',\n",
       " 'sky',\n",
       " 'excited',\n",
       " 'loose',\n",
       " 'unique',\n",
       " 'squad',\n",
       " 'compliment',\n",
       " 'baby',\n",
       " 'breathe',\n",
       " 'donation',\n",
       " 'productive',\n",
       " 'low',\n",
       " 'great',\n",
       " 'oh',\n",
       " 'custom',\n",
       " 'expensive',\n",
       " 'sand',\n",
       " 'lol',\n",
       " 'dang',\n",
       " 'smiles',\n",
       " 'flights',\n",
       " 'princess',\n",
       " 'snow',\n",
       " 'excellent',\n",
       " 'cereal',\n",
       " 'memories',\n",
       " 'lovin',\n",
       " 'fascinating',\n",
       " 'omg',\n",
       " 'ray',\n",
       " 'buyers',\n",
       " 'outstanding',\n",
       " 'luv',\n",
       " 'mountains',\n",
       " '#free',\n",
       " 'girl',\n",
       " 'bacon',\n",
       " 'ego',\n",
       " 'porn',\n",
       " 'okay',\n",
       " 'thrilled',\n",
       " 'useful',\n",
       " 'thankful',\n",
       " 'reward',\n",
       " 'snack',\n",
       " 'decent',\n",
       " 'describe',\n",
       " 'vintage',\n",
       " 'flower',\n",
       " 'recipe',\n",
       " 'sweat',\n",
       " 'mr',\n",
       " 'boy',\n",
       " 'buyer',\n",
       " 'fav',\n",
       " 'natural',\n",
       " 'sis',\n",
       " 'mighty',\n",
       " 'special',\n",
       " 'opener',\n",
       " 'flowers',\n",
       " 'harvest',\n",
       " 'hero',\n",
       " 'liking',\n",
       " 'ahhh',\n",
       " 'kings',\n",
       " 'spectacular',\n",
       " 'raining',\n",
       " 'lax',\n",
       " 'attractive',\n",
       " 'cooked',\n",
       " 'thanx',\n",
       " 'earrings',\n",
       " 'xd',\n",
       " 'sunny',\n",
       " 'trip',\n",
       " 'angels',\n",
       " 'yay',\n",
       " 'powerful',\n",
       " '#travel',\n",
       " '#shoutout',\n",
       " 'ohhh',\n",
       " 'affordable',\n",
       " 'rude',\n",
       " 'laura',\n",
       " 'honestly',\n",
       " 'hahahaha',\n",
       " 'lighting',\n",
       " 'chic',\n",
       " 'fans',\n",
       " 'hurting',\n",
       " 'lazy',\n",
       " 'dogs',\n",
       " 'buffet',\n",
       " 'shortly',\n",
       " 'ah',\n",
       " 'babe',\n",
       " 'garlic',\n",
       " 'slice',\n",
       " 'monkey',\n",
       " 'well',\n",
       " 'rainy',\n",
       " 'little',\n",
       " 'safely',\n",
       " 'slim',\n",
       " 'jerk',\n",
       " 'wassup',\n",
       " 'indeed',\n",
       " 'doe',\n",
       " 'basic',\n",
       " 'forward',\n",
       " 'classy',\n",
       " 'hey',\n",
       " 'octoberth',\n",
       " 'vacation',\n",
       " 'pink',\n",
       " 'tea',\n",
       " '#musicmonday',\n",
       " 'craving',\n",
       " 'robot',\n",
       " 'easily',\n",
       " 'vitamin',\n",
       " 'wisdom',\n",
       " 'stolen',\n",
       " 'troops',\n",
       " 'haha',\n",
       " 'chest',\n",
       " 'caffeine',\n",
       " 'birthday',\n",
       " '#photography',\n",
       " 'earn',\n",
       " 'bowl',\n",
       " 'inspired',\n",
       " 'wink',\n",
       " 'returned',\n",
       " 'mercury',\n",
       " 'pumpkin',\n",
       " 'remix',\n",
       " 'miss',\n",
       " 'crappy',\n",
       " 'blessing',\n",
       " 'billy',\n",
       " 'yum',\n",
       " 'significant',\n",
       " 'mars',\n",
       " 'small',\n",
       " 'dessert',\n",
       " 'grateful',\n",
       " 'champagne',\n",
       " 'recipes',\n",
       " 'creator',\n",
       " 'hun',\n",
       " 'carl',\n",
       " 'sir',\n",
       " 'pillow',\n",
       " 'yesss',\n",
       " 'brave',\n",
       " 'les',\n",
       " 'smart',\n",
       " 'rescue',\n",
       " 'dressing',\n",
       " 'kind',\n",
       " 'enjoy',\n",
       " 'ugly',\n",
       " 'wooo',\n",
       " 'raw',\n",
       " 'hahah',\n",
       " 'hangover',\n",
       " 'necklace',\n",
       " 'flying',\n",
       " 'troy',\n",
       " 'insane',\n",
       " 'northern',\n",
       " 'lemon',\n",
       " 'bound',\n",
       " 'soy',\n",
       " 'sin',\n",
       " 'sigh',\n",
       " 'douche',\n",
       " 'mention',\n",
       " 'tour',\n",
       " 'nasa',\n",
       " 'gosh',\n",
       " 'lips',\n",
       " 'weak',\n",
       " 'cole',\n",
       " 'autumn',\n",
       " 'ridiculous',\n",
       " 'unbelievable',\n",
       " 'fake',\n",
       " 'bruh',\n",
       " 'excuse',\n",
       " 'nephew',\n",
       " 'strength',\n",
       " 'fault',\n",
       " 'trail',\n",
       " 'extreme',\n",
       " 'soldier',\n",
       " 'defeat',\n",
       " 'smooth',\n",
       " 'dumb',\n",
       " 'hard',\n",
       " 'common',\n",
       " 'rice',\n",
       " 'greatness',\n",
       " 'desperate',\n",
       " 'grass',\n",
       " 'wack',\n",
       " 'train',\n",
       " 'happyth',\n",
       " 'gold',\n",
       " 'cart',\n",
       " 'authentic',\n",
       " 'cloudy',\n",
       " 'bloody',\n",
       " 'evil',\n",
       " 'cupcake',\n",
       " 'uploading',\n",
       " 'nom',\n",
       " 'lolz',\n",
       " 'returns',\n",
       " 'burrito',\n",
       " 'hehe',\n",
       " 'sweetie',\n",
       " 'hilton',\n",
       " 'stupid',\n",
       " 'tommy',\n",
       " 'blink',\n",
       " 'kiss',\n",
       " 'typical',\n",
       " 'sarah',\n",
       " 'messed',\n",
       " 'cab',\n",
       " 'gross',\n",
       " 'destroy',\n",
       " 'fried',\n",
       " '_',\n",
       " 'bonus',\n",
       " 'heaven',\n",
       " 'shine',\n",
       " 'crazy',\n",
       " 'badly',\n",
       " 'funeral',\n",
       " 'lauren',\n",
       " 'homie',\n",
       " 'thai',\n",
       " 'ultimate',\n",
       " 'exclusive',\n",
       " 'attention',\n",
       " 'damn',\n",
       " 'first',\n",
       " 'freakin',\n",
       " 'y',\n",
       " 'plane',\n",
       " 'danny',\n",
       " 'reserve',\n",
       " 'corps',\n",
       " 'shitty',\n",
       " 'careful',\n",
       " 'donate',\n",
       " 'gracias',\n",
       " 'exact',\n",
       " 'habit',\n",
       " 'mental',\n",
       " 'strong',\n",
       " 'joy',\n",
       " 'brew',\n",
       " 'sucks',\n",
       " 'falling',\n",
       " 'angeles',\n",
       " 'premium',\n",
       " 'value',\n",
       " 'idol',\n",
       " 'boi',\n",
       " 'essential',\n",
       " 'inspiration',\n",
       " 'anthem',\n",
       " 'impressed',\n",
       " 'favorite',\n",
       " 'shocking',\n",
       " 'mario',\n",
       " 'tours',\n",
       " 'dreams',\n",
       " 'football',\n",
       " 'enjoyed',\n",
       " 'lil',\n",
       " 'depressing',\n",
       " 'toast',\n",
       " 'shining',\n",
       " 'pancakes',\n",
       " 'ye',\n",
       " 'tan',\n",
       " 'quick',\n",
       " 'new',\n",
       " 'grilled',\n",
       " 'recovering',\n",
       " 'luxury',\n",
       " 'tacos',\n",
       " 'many',\n",
       " 'heal',\n",
       " 'christmas',\n",
       " 'brewing',\n",
       " 'magical',\n",
       " 'ja',\n",
       " 'save',\n",
       " 'heroes',\n",
       " 'cold',\n",
       " 'made',\n",
       " 'popcorn',\n",
       " 'gf',\n",
       " 'huge',\n",
       " 'deep',\n",
       " 'shouts',\n",
       " 'rich',\n",
       " 'maintenance',\n",
       " 'taste',\n",
       " 'jamie',\n",
       " 'page',\n",
       " '#twibbon',\n",
       " 'mother',\n",
       " 'lmfao',\n",
       " 'drag',\n",
       " 'surprised',\n",
       " 'amber',\n",
       " 'rain',\n",
       " 'velvet',\n",
       " 'laundry',\n",
       " 'walks',\n",
       " 'graphic',\n",
       " 'hug',\n",
       " 'slow',\n",
       " 'sara',\n",
       " 'por',\n",
       " 'lose',\n",
       " 'busy',\n",
       " 'twin',\n",
       " 'balls',\n",
       " 'bored',\n",
       " 'eyes',\n",
       " 'loved',\n",
       " '•',\n",
       " 'wedding',\n",
       " 'dust',\n",
       " 'glad',\n",
       " 'momma',\n",
       " 'native',\n",
       " 'deserved',\n",
       " 'promised',\n",
       " 'protect',\n",
       " 'cheers',\n",
       " 'backyard',\n",
       " 'awkward',\n",
       " 'high',\n",
       " 'spa',\n",
       " 'friend',\n",
       " 'invitation',\n",
       " 'official',\n",
       " 'precious',\n",
       " 'nite',\n",
       " 'label',\n",
       " 'sugar',\n",
       " 'chicken',\n",
       " 'woah',\n",
       " 'hopes',\n",
       " 'black',\n",
       " 'big',\n",
       " 'plate',\n",
       " 'method',\n",
       " 'deserves',\n",
       " 'prices',\n",
       " 'dream',\n",
       " 'optimization',\n",
       " 'onion',\n",
       " 'challenging',\n",
       " 'lion',\n",
       " 'valuable',\n",
       " 'premiere',\n",
       " 'serious',\n",
       " 'organic',\n",
       " 'recent',\n",
       " 'kindness',\n",
       " 'lost',\n",
       " 'cup',\n",
       " 'woot',\n",
       " 'purple',\n",
       " 'difficult',\n",
       " 'flag',\n",
       " 'ok',\n",
       " 'rocks',\n",
       " 'journey',\n",
       " 'bites',\n",
       " 'killer',\n",
       " 'winning',\n",
       " 'lift',\n",
       " 'victory',\n",
       " 'pics',\n",
       " 'makeover',\n",
       " 'handmade',\n",
       " 'mobile',\n",
       " 'shrimp',\n",
       " 'uses',\n",
       " 'britney',\n",
       " 'daughter',\n",
       " 'brought',\n",
       " 'loud',\n",
       " 'snap',\n",
       " 'homecoming',\n",
       " 'like',\n",
       " 'convince',\n",
       " 'secure',\n",
       " 'often',\n",
       " 'shuttle',\n",
       " 'lightning',\n",
       " 'comfortable',\n",
       " 'billionaire',\n",
       " 'stadium',\n",
       " 'iphones',\n",
       " 'gig',\n",
       " 'sponsor',\n",
       " 'bout',\n",
       " 'pictures',\n",
       " 'winner',\n",
       " 'spider',\n",
       " 'adventure',\n",
       " 'surfing',\n",
       " 'brush',\n",
       " 'minus',\n",
       " 'sure',\n",
       " 'orange',\n",
       " 'fair',\n",
       " 'craig',\n",
       " 'modern',\n",
       " '#blackfriday',\n",
       " 'submitted',\n",
       " '#mm',\n",
       " 'relevant',\n",
       " 'gratitude',\n",
       " 'fast',\n",
       " 'friends',\n",
       " 'shocked',\n",
       " 'leather',\n",
       " 'lady',\n",
       " 'depressed',\n",
       " 'tap',\n",
       " 'deadly',\n",
       " 'bud',\n",
       " 'bold',\n",
       " 'resort',\n",
       " 'quality',\n",
       " 'showers',\n",
       " 'coupons',\n",
       " 'tu',\n",
       " 'kitty',\n",
       " 'inspire',\n",
       " 'connected',\n",
       " 'roses',\n",
       " 'bake',\n",
       " 'captain',\n",
       " 'scratch',\n",
       " 'mothers',\n",
       " 'ninja',\n",
       " 'goodmorning',\n",
       " 'rides',\n",
       " 'heavy',\n",
       " 'profit',\n",
       " 'come',\n",
       " 'tomato',\n",
       " 'supreme',\n",
       " 'navy',\n",
       " 'saved',\n",
       " 'uh',\n",
       " 'supposed',\n",
       " 'someday',\n",
       " 'hungry',\n",
       " 'arrived',\n",
       " 'sea',\n",
       " 'rio',\n",
       " 'jonathan',\n",
       " 'trey',\n",
       " 'ha',\n",
       " 'secret',\n",
       " 'happiness',\n",
       " 'photos',\n",
       " 'surprise',\n",
       " 'pour',\n",
       " 'steak',\n",
       " 'fucking',\n",
       " 'alone',\n",
       " 'continued',\n",
       " 'misses',\n",
       " 'sight',\n",
       " '#wine',\n",
       " 'based',\n",
       " 'td',\n",
       " 'angel',\n",
       " 'delta',\n",
       " 'makes',\n",
       " 'got',\n",
       " 'god',\n",
       " 'nissan',\n",
       " 'horrible',\n",
       " 'aw',\n",
       " 'gucci',\n",
       " '#etsy',\n",
       " 'touched',\n",
       " 'sexual',\n",
       " 'ghetto',\n",
       " 'cow',\n",
       " 'jacket',\n",
       " 'sings',\n",
       " 'field',\n",
       " 'brother',\n",
       " 'heather',\n",
       " 'fully',\n",
       " 'ya',\n",
       " 'diva',\n",
       " 'fog',\n",
       " 'active',\n",
       " 'cali',\n",
       " 'strawberry',\n",
       " 'leaf',\n",
       " 'note',\n",
       " 'love',\n",
       " 'jk',\n",
       " 'description',\n",
       " 'soldiers',\n",
       " 'vw',\n",
       " 'skinny',\n",
       " 'bunny',\n",
       " 'zig',\n",
       " 'sunshine',\n",
       " 'goodness',\n",
       " 'como',\n",
       " 'mama',\n",
       " 'bbq',\n",
       " 'poop',\n",
       " 'legs',\n",
       " 'surprising',\n",
       " 'tigers',\n",
       " 'limited',\n",
       " 'fucked',\n",
       " 'pain',\n",
       " 'bless',\n",
       " 'plastic',\n",
       " 'gods',\n",
       " 'chip',\n",
       " 'photographer',\n",
       " 'lovers',\n",
       " 'travels',\n",
       " 'chad',\n",
       " 'josh',\n",
       " 'sisters',\n",
       " 'mans',\n",
       " 'xmas',\n",
       " 'surprisingly',\n",
       " 'entertaining',\n",
       " 'vanilla',\n",
       " 'lmao',\n",
       " 'comfort',\n",
       " 'darn',\n",
       " 'blonde',\n",
       " 'ba',\n",
       " 'joel',\n",
       " 'large',\n",
       " 'shall',\n",
       " 'lou',\n",
       " 'furniture',\n",
       " 'solid',\n",
       " 'clouds',\n",
       " 'close',\n",
       " 'ride',\n",
       " 'premier',\n",
       " 'cyber',\n",
       " 'nick',\n",
       " 'pie',\n",
       " 'festival',\n",
       " 'wings',\n",
       " 'demi',\n",
       " 'tone',\n",
       " '¸',\n",
       " 'hidden',\n",
       " 'mayer',\n",
       " 'brings',\n",
       " 'promises',\n",
       " 'emotional',\n",
       " 'random',\n",
       " 'monster',\n",
       " 'surf',\n",
       " 'donuts',\n",
       " 'wines',\n",
       " 'whitening',\n",
       " 'sample',\n",
       " 'ace',\n",
       " 'tribute',\n",
       " 'stuffed',\n",
       " 'deserve',\n",
       " 'volleyball',\n",
       " 'frustrating',\n",
       " 'earned',\n",
       " 'michelle',\n",
       " 'mrs',\n",
       " 'profiles',\n",
       " 'negative',\n",
       " 'bye',\n",
       " 'express',\n",
       " 'juice',\n",
       " 'dads',\n",
       " 'anxiety',\n",
       " 'shake',\n",
       " 'referring',\n",
       " 'evening',\n",
       " 'fave',\n",
       " 'provided',\n",
       " 'bugs',\n",
       " 'youtube',\n",
       " 'para',\n",
       " 'rachel',\n",
       " 'accomplished',\n",
       " 'waves',\n",
       " 'sorry',\n",
       " 'lover',\n",
       " 'shoutout',\n",
       " 'ass',\n",
       " 'tweet',\n",
       " 'neighbors',\n",
       " 'classic',\n",
       " 'remind',\n",
       " 'lmaooo',\n",
       " 'comin',\n",
       " 'mms',\n",
       " 'jet',\n",
       " 'beers',\n",
       " 'yo',\n",
       " 'appeal',\n",
       " 'ruin',\n",
       " 'likely',\n",
       " 'pumped',\n",
       " 'win',\n",
       " 'pissed',\n",
       " 'auf',\n",
       " 'personal',\n",
       " 'pretty',\n",
       " 'miley',\n",
       " 'death',\n",
       " 'maker',\n",
       " 'young',\n",
       " 'obsessed',\n",
       " 'mint',\n",
       " 'tide',\n",
       " 'si',\n",
       " 'whitney',\n",
       " 'cakes',\n",
       " 'crossed',\n",
       " 'machine',\n",
       " 'mia',\n",
       " 'everytime',\n",
       " 'nap',\n",
       " 'nerd',\n",
       " 'theater',\n",
       " 'royal',\n",
       " 'x',\n",
       " 'airport',\n",
       " 'band',\n",
       " 'lo',\n",
       " 'proverb',\n",
       " 'jason',\n",
       " 'heh',\n",
       " 'unexpected',\n",
       " 'terrific',\n",
       " 'whoever',\n",
       " 'vampire',\n",
       " 'seasonal',\n",
       " 'jack',\n",
       " 'breath',\n",
       " 'operation',\n",
       " 'headache',\n",
       " 'effective',\n",
       " 'rainbow',\n",
       " 'course',\n",
       " 'forget',\n",
       " 'saint',\n",
       " 'trips',\n",
       " 'style',\n",
       " 'makin',\n",
       " 'courtesy',\n",
       " 'england',\n",
       " 'ooh',\n",
       " 'tweeps',\n",
       " 'adventures',\n",
       " 'mommy',\n",
       " 'wishes',\n",
       " 'photography',\n",
       " 'devil',\n",
       " 'acoustic',\n",
       " 'septemberth',\n",
       " 'green',\n",
       " 'far',\n",
       " 'reggae',\n",
       " '#ff',\n",
       " 'ladies',\n",
       " 'paradise',\n",
       " 'plz',\n",
       " 'retarded',\n",
       " 'appreciate',\n",
       " 'eaten',\n",
       " 'burning',\n",
       " 'inner',\n",
       " 'lyrics',\n",
       " 'funniest',\n",
       " 'piss',\n",
       " 'gr',\n",
       " 'above',\n",
       " 'nooo',\n",
       " 'eric',\n",
       " 'fallen',\n",
       " 'fam',\n",
       " 'cookies',\n",
       " 'puppy',\n",
       " 'isn',\n",
       " 'turkey',\n",
       " 'andy',\n",
       " '#ad',\n",
       " 'aids',\n",
       " 'healing',\n",
       " 'holy',\n",
       " 'border',\n",
       " 'honored',\n",
       " 'soccer',\n",
       " 'brothers',\n",
       " 'landed',\n",
       " 'ooo',\n",
       " 'liked',\n",
       " 'smell',\n",
       " 'rarely',\n",
       " 'cure',\n",
       " 'awful',\n",
       " 'rev',\n",
       " 'dig',\n",
       " 'proof',\n",
       " 'search',\n",
       " 'accepted',\n",
       " 'rent',\n",
       " 'tight',\n",
       " 'recap',\n",
       " 'le',\n",
       " 'bass',\n",
       " 'holiday',\n",
       " 'escape',\n",
       " 'accepting',\n",
       " 'treasure',\n",
       " 'afternoon',\n",
       " 'palm',\n",
       " 'forgot',\n",
       " 'pandora',\n",
       " 'republic',\n",
       " 'prepared',\n",
       " 'jen',\n",
       " 'steal',\n",
       " 'thunder',\n",
       " 'counter',\n",
       " 'sees',\n",
       " 'shout',\n",
       " 'butt',\n",
       " 'land',\n",
       " 'nope',\n",
       " 'boyfriend',\n",
       " 'concerned',\n",
       " 'grow',\n",
       " 'dad',\n",
       " 'hip',\n",
       " 'stretch',\n",
       " 'devils',\n",
       " 'frozen',\n",
       " 'complicated',\n",
       " 'janet',\n",
       " 'steel',\n",
       " 'xxx',\n",
       " 'congrats',\n",
       " 'bro',\n",
       " 'butter',\n",
       " 'pussy',\n",
       " 'sake',\n",
       " 'gemini',\n",
       " 'does',\n",
       " 'casino',\n",
       " 'dark',\n",
       " 'drunk',\n",
       " 'shipping',\n",
       " 'buddy',\n",
       " 'haters',\n",
       " 'owl',\n",
       " 'smash',\n",
       " 'aware',\n",
       " 'floyd',\n",
       " 'veterans',\n",
       " 'pirate',\n",
       " 'bon',\n",
       " 'font',\n",
       " 'hee',\n",
       " 'giveaway',\n",
       " 'handy',\n",
       " 'hubby',\n",
       " 'guilty',\n",
       " 'rick',\n",
       " 'bedroom',\n",
       " 'thx',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmodel.sort(\"sweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(architecture='cbow', batch_size=128,\n",
       "         count=Counter({'UNK': 6334388, 'the': 1415963, 'to': 1117395,\n",
       "                        'a': 884380, 'i': 821540, 'and': 624171, 'for': 591703,\n",
       "                        'in': 585901, 'of': 577948, 'you': 572544, 'is': 523046,\n",
       "                        'on': 476378, 'it': 387693, 'my': 361536, 'rt': 348880,\n",
       "                        'at': 310664, 'that': 275618, 'with': 261940,\n",
       "                        'this': 253464, 'your': 229211, 'be': 217495,\n",
       "                        'me': 213922, 'just': 206190...\n",
       "         reverse_dictionary={0: 'UNK', 1: 'ama', 2: 'lesson', 3: 'doll',\n",
       "                             4: 'soy', 5: 'charlotte', 6: 'issue', 7: 'flight',\n",
       "                             8: 'lawn', 9: 'demi', 10: 'street', 11: 'drops',\n",
       "                             12: 'finally', 13: '#ff', 14: 'theatre',\n",
       "                             15: 'enjoyed', 16: 'winner', 17: 'brands',\n",
       "                             18: 'community', 19: 'though', 20: 'cyber',\n",
       "                             21: 'appears', 22: 'critical', 23: 'titans',\n",
       "                             24: 'hell', 25: 'eagles', 26: 'blew', 27: 'kansas',\n",
       "                             28: 'wondered', 29: 'checking', ...},\n",
       "         skip_window=2, vocabulary_size=6000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmodel.epochs = 10\n",
    "pmodel.batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rando_data = local_models.tf_w2v_models.build_dataset_predictionary(\n",
    "    [\"the quick brown fox jumps over the lazy dog\".split(\" \")],\n",
    "    pmodel.dictionary, pmodel.reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4684, 3350, 4985, 91, 0, 746, 4684, 1592, 4099]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rando_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(architecture='cbow', batch_size=1,\n",
       "         count=Counter({'UNK': 6334388, 'the': 1415963, 'to': 1117395,\n",
       "                        'a': 884380, 'i': 821540, 'and': 624171, 'for': 591703,\n",
       "                        'in': 585901, 'of': 577948, 'you': 572544, 'is': 523046,\n",
       "                        'on': 476378, 'it': 387693, 'my': 361536, 'rt': 348880,\n",
       "                        'at': 310664, 'that': 275618, 'with': 261940,\n",
       "                        'this': 253464, 'your': 229211, 'be': 217495,\n",
       "                        'me': 213922, 'just': 206190, '...\n",
       "         reverse_dictionary={0: 'UNK', 1: 'ama', 2: 'lesson', 3: 'doll',\n",
       "                             4: 'soy', 5: 'charlotte', 6: 'issue', 7: 'flight',\n",
       "                             8: 'lawn', 9: 'demi', 10: 'street', 11: 'drops',\n",
       "                             12: 'finally', 13: '#ff', 14: 'theatre',\n",
       "                             15: 'enjoyed', 16: 'winner', 17: 'brands',\n",
       "                             18: 'community', 19: 'though', 20: 'cyber',\n",
       "                             21: 'appears', 22: 'critical', 23: 'titans',\n",
       "                             24: 'hell', 25: 'eagles', 26: 'blew', 27: 'kansas',\n",
       "                             28: 'wondered', 29: 'checking', ...},\n",
       "         skip_window=2, vocabulary_size=6000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmodel.fit(rando_data, sample_weight=np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
