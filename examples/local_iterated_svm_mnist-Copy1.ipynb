{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import local_models.local_models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.cluster\n",
    "import sklearn.svm\n",
    "from importlib import reload\n",
    "from ml_battery.utils import cmap\n",
    "import matplotlib as mpl\n",
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "import logging\n",
    "import ml_battery.log\n",
    "import time\n",
    "import os\n",
    "import functools\n",
    "import collections\n",
    "import joblib\n",
    "import local_models.loggin\n",
    "import local_models.TLS_models\n",
    "import local_models.utils\n",
    "from local_models.TLS_models import LinearRegression, QuadraticRegression, SphericalRegression, LinearODR\n",
    "from sklearn import datasets, svm, metrics\n",
    "import cv2\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "np.random.seed(1)\n",
    "#reload(local_models.loggin)\n",
    "#reload(local_models.TLS_models)\n",
    "np.warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = [8.0, 8.0]\n",
    "font = {'size'   : 18}\n",
    "\n",
    "mpl.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = sklearn.datasets.fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.shape, mnist.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_resized = np.stack([cv2.resize(mnist.data[i].reshape(28, 28), (8,8)) for i in range(mnist.data.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_resized = mnist_resized.reshape(mnist.data.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = 4\n",
    "project_dir = \"../data/local_svm_mnist_{:02d}\".format(RUN)\n",
    "os.makedirs(project_dir, exist_ok=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_and_labels = list(zip(mnist.data, mnist.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f37296aec18>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('{}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = mnist.data.shape[0]\n",
    "data = mnist.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1\n",
    "subsampled_data, _, subsampled_labels, _ = sklearn.model_selection.train_test_split(\n",
    "    data, mnist.target, test_size=1-subsample_rate, random_state=SEED, stratify=mnist.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1\n",
    "train_data, test_data, train_labels, test_labels = sklearn.model_selection.train_test_split(\n",
    "    subsampled_data, subsampled_labels, test_size=0.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.svm.LinearSVC(C=3000., dual=False)\n",
    "index = sklearn.neighbors.KDTree(train_data, leaf_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<local_models.local_models.LocalModels at 0x7f3728bc5048>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_models = local_models.local_models.LocalModels(model)\n",
    "linear_models.fit(train_data, train_labels, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz, iz = linear_models.index.query(train_data, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_1nn_dist = np.average(dz[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = local_models.local_models.TriCubeKernel(bandwidth=avg_1nn_dist*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4900, 784)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4900,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reject_pointwise_bases(x, bases, mean=0):\n",
    "    x = x - mean #mean center everything\n",
    "    projection = local_models.utils.linear_project_pointwise_bases(x, bases)\n",
    "    rejection = x - projection\n",
    "    rejection = rejection + mean #re-add the mean in\n",
    "    return rejection\n",
    "\n",
    "def scms(X, lm, kernel, iters=30, constraint_space=None, return_params=False, failure_delta=None):\n",
    "    #all_failures = []\n",
    "    \n",
    "    timelog = local_models.loggin.TimeLogger(logger=logger, how_often=1, total=iters, tag=\"local_svm_shifter\")\n",
    "\n",
    "    if failure_delta is None:\n",
    "        failure_delta = np.average(lm.index.query(X, k=2)[0][:,1])*1e4\n",
    "    for i in range(iters):\n",
    "        with timelog:\n",
    "            X = np.copy(X)\n",
    "            Xrange = np.arange(X.shape[0])\n",
    "            params = lm.transform(X, r=kernel.support_radius(), weighted=True,\n",
    "                kernel=kernel)\n",
    "            logger.info(\"svm_params_transformed!\")\n",
    "            normalized_params = params/np.sqrt(np.sum(params[:,:X.shape[1]]**2,axis=-1,keepdims=True))\n",
    "            normals = normalized_params[:,:X.shape[1]]\n",
    "            intercepts = normalized_params[:,X.shape[1]]\n",
    "            biggest_normal_component = np.argmax(normals, axis=1)\n",
    "            biggest_normal_component_indices = np.stack((Xrange, biggest_normal_component))\n",
    "            biggest_normal_component_indices = tuple(map(tuple, biggest_normal_component_indices))\n",
    "\n",
    "            plane_pt_component = -intercepts/normalized_params[biggest_normal_component_indices]\n",
    "            plane_pts = np.zeros(normals.shape)\n",
    "            plane_pts[biggest_normal_component_indices] = plane_pt_component\n",
    "\n",
    "            normals = normals.reshape(X.shape[0], 1, X.shape[1])\n",
    "            new_X = linear_reject_pointwise_bases(X, normals, plane_pts)\n",
    "            failures = np.sqrt(np.sum((new_X-X)**2, axis=1)) > failure_delta\n",
    "            successes = np.logical_not(failures)\n",
    "            X[successes] = new_X[successes]\n",
    "            if constraint_space is not None:\n",
    "                X[successes] = local_models.utils.linear_project_pointwise_bases(X[successes], constraint_space[0][successes], constraint_space[1][successes])\n",
    "\n",
    "            if return_params:\n",
    "                yield X, successes, normals\n",
    "            else:\n",
    "                yield X, successes\n",
    "\n",
    "def exhaust(gen):\n",
    "    def exhauster(*args, **kwargs):\n",
    "        for _ in gen(*args, **kwargs): pass\n",
    "        return _\n",
    "    return exhauster\n",
    "\n",
    "import tempfile\n",
    "import time\n",
    "def scms_parallel_sharedmem(X, lm, kernel, iters=30, constraint_space=None, return_params=False, failure_delta=None, batch_size=100):\n",
    "    batches = (np.array([0, batch_size]) + batch_size*i for i in range(int(np.ceil(X.shape[0]/batch_size))))\n",
    "    with tempfile.NamedTemporaryFile(dir=\"/dev/shm\") as shared_X_ramspace, tempfile.NamedTemporaryFile(dir=\"/dev/shm\") as shared_constraint0_ramspace, tempfile.NamedTemporaryFile(dir=\"/dev/shm\") as shared_constraint1_ramspace:\n",
    "        shared_X = np.memmap(shared_X_ramspace, dtype=X.dtype,\n",
    "                   shape=X.shape, mode='w+')\n",
    "        shared_X[:] = X[:]\n",
    "        if constraint_space is not None:\n",
    "            shared_constraint_space0 = np.memmap(shared_constraint0_ramspace, dtype=constraint_space[0].dtype,\n",
    "                   shape=constraint_space[0].shape, mode='w+')\n",
    "            shared_constraint_space1 = np.memmap(shared_constraint1_ramspace, dtype=constraint_space[1].dtype,\n",
    "                   shape=constraint_space[1].shape, mode='w+')\n",
    "            shared_constraint_space0[:] = constraint_space[0][:]\n",
    "            shared_constraint_space1[:] = constraint_space[1][:]\n",
    "            constraint_space = (shared_constraint_space0, shared_constraint_space1)\n",
    "        parallel_sols = joblib.Parallel(n_jobs=12)(joblib.delayed(exhaust(scms))(\n",
    "            shared_X[slice(*batch)], lm, kernel, iters, \n",
    "            None if constraint_space is None else tuple(map(lambda c: c[slice(*batch)], constraint_space)), \n",
    "            return_params, failure_delta)\n",
    "            for batch in batches)\n",
    "    res = tuple(map(functools.partial(np.concatenate, axis=0), zip(*parallel_sols)))\n",
    "    yield res\n",
    "    \n",
    "def scms_parallel(X, lm, kernel, iters=30, constraint_space=None, return_params=False, failure_delta=None, \n",
    "        n_jobs=24, batch_size=100):\n",
    "    batches = (np.array([0, batch_size]) + batch_size*i for i in range(int(np.ceil(X.shape[0]/batch_size))))\n",
    "    parallel_sols = joblib.Parallel(n_jobs=n_jobs)(joblib.delayed(exhaust(scms))(\n",
    "        X[slice(*batch)], lm, kernel, iters, \n",
    "        None if constraint_space is None else tuple(map(lambda c: c[slice(*batch)], constraint_space)), \n",
    "        return_params, failure_delta)\n",
    "        for batch in batches)\n",
    "    res = tuple(map(functools.partial(np.concatenate, axis=0), zip(*parallel_sols)))\n",
    "    yield res\n",
    "\n",
    "import traceback\n",
    "import collections\n",
    "def orthogonal_project_scms(X, lm, kernel, scms_iters=30, newtons_iters=30, alpha=1e-2, return_everything=False, \n",
    "    parallel=False, sharedmem=True, n_jobs=24, batch_size=100):\n",
    "    #1. do scms to get *a* point on the surface, y\n",
    "    #2. get the tangent plane at y\n",
    "    scms_method = scms_parallel_sharedmem if (parallel and sharedmem) else (scms_parallel if parallel else scms)\n",
    "\n",
    "    newton_timelog = local_models.loggin.TimeLogger(logger=logger, how_often=1, total=newtons_iters, tag=\"local_svm_newton\")\n",
    "    shifter_timelog = local_models.loggin.TimeLogger(logger=logger, how_often=1, total=scms_iters, tag=\"local_svm_shift\")\n",
    "\n",
    "    logger.info(\"svm shifting!\")\n",
    "    if return_everything:\n",
    "        everything = collections.defaultdict(list)\n",
    "    shifter = scms_method(X,lm,kernel,iters=scms_iters,return_params=True, n_jobs=n_jobs, batch_size=batch_size)\n",
    "    for y, successes, normals in shifter:\n",
    "        with shifter_timelog:\n",
    "            if return_everything:\n",
    "                everything[0].append((y, successes, normals))\n",
    "    shifter_timelog.i=0\n",
    "    logger.info(\"svm shifted!\")\n",
    "    X = X[successes]\n",
    "    y = y[successes]\n",
    "    normals = normals[successes]\n",
    "    #3. do scms while projecting along some convex combo of the line passing thru x and y, and \n",
    "    #   the line passing through x and along the normal vector to the tangent plane in 2 to get y'\n",
    "    #4. y <- y'\n",
    "    #5. GOTO 2\n",
    "    for i in range(newtons_iters):\n",
    "        with newton_timelog:\n",
    "            try:\n",
    "                Xy = y-X\n",
    "                normalized_Xy = (Xy)/np.sqrt(np.sum(Xy**2,axis=1,keepdims=True))\n",
    "                normalized_Xy = np.expand_dims(normalized_Xy, 1)\n",
    "                #print(\"shapes\", normalized_Xy.shape, normals.shape)\n",
    "                surface_normal_aligned_Xy = normalized_Xy * np.sign(np.sum(normalized_Xy*normals, axis=-1, keepdims=True))\n",
    "                constraint_vec = surface_normal_aligned_Xy*(1-alpha) + normals*alpha\n",
    "                constraint_vec = constraint_vec/np.sqrt(np.sum(constraint_vec**2, axis=-1, keepdims=True))\n",
    "                print(\"constraint shape\", constraint_vec.shape)\n",
    "                shifter = scms_method(X,lm,kernel,iters=scms_iters,return_params=True,\n",
    "                    constraint_space=(constraint_vec, X), n_jobs=n_jobs, batch_size=batch_size)\n",
    "                for y, successes, normals in shifter:\n",
    "                    with shifter_timelog:\n",
    "                        if return_everything:\n",
    "                            everything[i+1].append((y, successes, normals))\n",
    "                shifter_timelog.i=0\n",
    "                X = X[successes]\n",
    "                y = y[successes]\n",
    "                normals = normals[successes]\n",
    "            except:\n",
    "                logger.info(traceback.print_exc())\n",
    "                break\n",
    "    if return_everything:\n",
    "        return everything\n",
    "    return X, y, normals  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class SVM_SCMS_Classifier(sklearn.base.ClassifierMixin):\n",
    "    def __init__(self, penalty='l2', loss='squared_hinge', dual=True, \n",
    "        tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, \n",
    "        intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000,\n",
    "        kernel=None):\n",
    "        \n",
    "        self.penalty=penalty\n",
    "        self.loss=loss\n",
    "        self.dual=dual\n",
    "        self.tol=tol\n",
    "        self.C=C\n",
    "        self.multi_class=multi_class\n",
    "        self.fit_intercept=fit_intercept\n",
    "        self.intercept_scaling=intercept_scaling\n",
    "        self.class_weight=class_weight\n",
    "        self.verbose=verbose\n",
    "        self.random_state=random_state\n",
    "        self.max_iter=max_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.model = sklearn.svm.SVC(penalty=self.penalty, loss=self.loss, dual=self.dual,\n",
    "            tol=self.tol, C=self.C, multi_class=self.multi_class, fit_intercept=self.fit_intercept, \n",
    "            intercept_scaling=self.intercept_scaling, class_weight=self.class_weight, verbose=self.verbose, \n",
    "            random_state=self.random_state, max_iter=self.max_iter)\n",
    "        self.lmodels = local_models.local_models.LocalModels(self.model)\n",
    "        self.lmodels.fit(X, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_onehotifier = sklearn.preprocessing.OneHotEncoder([list(range(10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categories=[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], drop=None,\n",
       "              dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "              sparse=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_onehotifier.fit(np.array([list(map(str,range(10)))]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4900x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4900 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_labels = mnist_onehotifier.transform(np.expand_dims(train_labels, 1))\n",
    "onehot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '8', '1', ..., '6', '1', '0'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_labels = onehot_labels.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "models_dir = os.path.join(project_dir, \"ovr_models_b{:08.04f}_C{:08.04f}\".format(kernel.bandwidth,model.C))\n",
    "os.makedirs(models_dir, exist_ok=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "ovr_local_models = []\n",
    "for i in range(onehot_labels.shape[1]):\n",
    "    print(i)\n",
    "    ovr_local_models.append(local_models.local_models.LocalModels(model))\n",
    "    ovr_local_models[-1].fit(train_data, onehot_labels[:,i], index=index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': <sklearn.neighbors._kd_tree.KDTree at 0x6b1d498>,\n",
       " 'k': None,\n",
       " 'model': LinearSVC(C=3000.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       " 'model_features': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'model_targets': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " 'r': None,\n",
       " 'sample_weight': None}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovr_local_models[7].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 477.0\n",
      "1 549.0\n",
      "2 480.0\n",
      "3 527.0\n",
      "4 479.0\n",
      "5 457.0\n",
      "6 460.0\n",
      "7 500.0\n",
      "8 479.0\n",
      "9 492.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ovr_local_models)):\n",
    "    print(i, np.sum(ovr_local_models[i].model_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': <sklearn.neighbors._kd_tree.KDTree at 0x6b1d498>,\n",
       " 'k': None,\n",
       " 'model': LinearSVC(C=3000.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "           multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "           verbose=0),\n",
       " 'model_features': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'model_targets': array([0., 1., 0., ..., 0., 0., 0.]),\n",
       " 'r': None,\n",
       " 'sample_weight': None}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovr_local_models[8].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(train_data.shape[0]/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "train_predictions = []\n",
    "for i in range(4,onehot_labels.shape[1]):\n",
    "    print(i)\n",
    "    train_predictions.append(\n",
    "        orthogonal_project_scms(train_data, ovr_local_models[i], kernel, return_everything=False, alpha=0.3,\n",
    "            parallel=True, sharedmem=False, n_jobs=24, batch_size=int(np.ceil(train_data.shape[0]/24)))\n",
    "    )\n",
    "    X,y,normals = train_predictions[-1]\n",
    "    print(X.shape, y.shape, normals.shape)\n",
    "    logger.info(\"fitted mnist ovr model {}\".format(i))\n",
    "    np.savetxt(os.path.join(models_dir, \"ovr_model_predictions_X_{:03d}\".format(i)), X)\n",
    "    np.savetxt(os.path.join(models_dir, \"ovr_model_predictions_y_{:03d}\".format(i)), y)\n",
    "    np.savetxt(os.path.join(models_dir, \"ovr_model_predictions_normals_{:03d}\".format(i)), normals[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = []\n",
    "for i in range(onehot_labels.shape[1]):\n",
    "    X = np.loadtxt(os.path.join(models_dir, \"ovr_model_predictions_X_{:03d}\".format(i)))\n",
    "    y = np.loadtxt(os.path.join(models_dir, \"ovr_model_predictions_y_{:03d}\".format(i)))\n",
    "    normals = np.loadtxt(os.path.join(models_dir, \"ovr_model_predictions_normals_{:03d}\".format(i)))\n",
    "    train_predictions.append((X,y,normals))\n",
    "    print(X.shape, y.shape, normals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_labels = []\n",
    "train_pred_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_local_models[8].model_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ovr_local_models[8].model_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "preds = []\n",
    "scores = []\n",
    "for i in range(onehot_labels.shape[1]):\n",
    "    logger.info(\"training_scores {} {}\".format(i, (time.time() - start)/3600))\n",
    "    preds.append(ovr_local_models[i].predict(train_data, kernel=kernel, r=kernel.support_radius()))\n",
    "    scores.append(np.sqrt(np.sum((preds[i][0] - preds[i][1])**2, axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for i in range(onehot_labels.shape[1]):\n",
    "    test_predictions.append(\n",
    "        orthogonal_project_scms(test_data, ovr_local_models[i], kernel, return_everything=False, alpha=0.3, parallel=True, sharedmem=False)\n",
    "    )\n",
    "    X,y,normals = test_predictions[-1]\n",
    "    print(X.shape, y.shape, normals.shape)\n",
    "    logger.info(\"fitted mnist test ovr model {}\".format(i))\n",
    "    np.savetxt(os.path.join(models_dir, \"ovr_model_test_predictions_X_{:03d}\".format(i)), X)\n",
    "    np.savetxt(os.path.join(models_dir, \"ovr_model_test_predictions_y_{:03d}\".format(i)), y)\n",
    "    np.savetxt(os.path.join(models_dir, \"ovr_model_test_predictions_normals_{:03d}\".format(i)), normals[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(onehot_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_predictions), len(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Xy = (test_predictions[0][0] - test_predictions[0][1])\n",
    "test_Xy_normalized = test_Xy/np.sqrt(np.sum(test_Xy**2, axis=1,keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Xy = (train_predictions[0][0] - train_predictions[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(np.sum(test_predictions[0][2][:,0,:]*test_Xy_normalized, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platt_regularizers = []\n",
    "for i, ovr_local_model in enumerate(ovr_local_models):\n",
    "    print(i)\n",
    "    platt_regularizers.append(sklearn.linear_model.LogisticRegression(C=1e-2))\n",
    "    train_Xy = train_predictions[i][0] - train_predictions[i][1]\n",
    "    train_Xy_len = np.sqrt(np.sum(train_Xy**2, axis=1))\n",
    "    train_pred = ovr_local_model.predict(train_data, kernel=kernel, r=kernel.support_radius())\n",
    "    regularizer_input = train_Xy_len*(train_pred[:,0] * 2 - 1)\n",
    "    platt_regularizers[-1].fit(regularizer_input.reshape(-1,1), onehot_labels[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(regularizer_input, onehot_labels[:,i])\n",
    "grid = np.linspace(np.min(regularizer_input), np.max(regularizer_input), 100)\n",
    "plt.plot(grid, platt_regularizers[-1].predict_proba(grid.reshape(-1,1))[:,1], c='r')\n",
    "plt.xlabel(\"orthogonal distance to decision surface\", size=22)\n",
    "plt.ylabel(\"$P$\", size=22)\n",
    "plt.savefig("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_labels_test = mnist_onehotifier.transform(np.expand_dims(test_labels, 1)).toarray()\n",
    "onehot_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Xy = test_predictions[i][0] - test_predictions[i][1]\n",
    "test_Xy_len = np.sqrt(np.sum(test_Xy**2, axis=1))\n",
    "test_pred = ovr_local_model.predict(test_data, kernel=kernel, r=kernel.support_radius())\n",
    "regularizer_input_test = test_Xy_len*(test_pred[:,0] * 2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(regularizer_input_test, onehot_labels_test[:,i])\n",
    "grid = np.linspace(np.min(regularizer_input_test), np.max(regularizer_input_test), 100)\n",
    "plt.plot(grid, platt_regularizers[-1].predict_proba(grid.reshape(-1,1))[:,1], c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_output = []\n",
    "for i, ovr_local_model in enumerate(ovr_local_models):\n",
    "    print(i)\n",
    "    test_Xy = test_predictions[i][0] - test_predictions[i][1]\n",
    "    test_Xy_len = np.sqrt(np.sum(test_Xy**2, axis=1))\n",
    "    test_pred = ovr_local_model.predict(test_data, kernel=kernel, r=kernel.support_radius())\n",
    "    regularizer_input_test = test_Xy_len*(test_pred[:,0] * 2 - 1)\n",
    "    regularized_output.append(platt_regularizers[i].predict_proba(regularizer_input_test.reshape(-1,1))[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_output = np.stack(regularized_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_predictions_test = np.argmax(regularized_output, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "confusion_test = sklearn.metrics.confusion_matrix(test_labels, hard_predictions_test)\n",
    "confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall = (\n",
    "    sklearn.metrics.accuracy_score(test_labels, hard_predictions_test),\n",
    "    sklearn.metrics.precision_score(test_labels, hard_predictions_test, average=\"macro\"), \n",
    "    sklearn.metrics.recall_score(test_labels, hard_predictions_test, average=\"macro\"))\n",
    "accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonorthopred_transformator(m,q,x,y,w):\n",
    "    scores = m.decision_function(x)\n",
    "    preds = m.predict(x)\n",
    "    regularizer_features = scores*(preds*2-1)\n",
    "    regularizer = sklearn.linear_model.LogisticRegression(C=1e-2, fit_intercept=False)\n",
    "    regularizer.fit(regularizer_features.reshape(-1,1), y, sample_weight=w)\n",
    "    return regularizer.predict_proba(m.decision_function(q.reshape(1,-1)).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonortho_proba_transformations = []\n",
    "for i, ovr_local_model in enumerate(ovr_local_models):\n",
    "    nonortho_proba_transformations.append(\n",
    "        ovr_local_model.transform(test_data, kernel=kernel, r=kernel.support_radius(), weighted=True,\n",
    "            model_postprocessor=nonorthopred_transformator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonortho_proba_transformations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonortho_proba_transformations = np.stack(nonortho_proba_transformations, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonortho_proba_transformations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonortho_hard_predictions = np.argmax(nonortho_proba_transformations[:,0], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonortho_confusion = sklearn.metrics.confusion_matrix(test_labels, nonortho_hard_predictions)\n",
    "nonortho_acc, nonortho_prec, nonortho_rec = (\n",
    "    sklearn.metrics.accuracy_score(test_labels, nonortho_hard_predictions),\n",
    "    sklearn.metrics.precision_score(test_labels, nonortho_hard_predictions, average=\"macro\"), \n",
    "    sklearn.metrics.recall_score(test_labels, nonortho_hard_predictions, average=\"macro\"))\n",
    "nonortho_confusion, nonortho_acc, nonortho_prec, nonortho_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
